@article{Dwivedi2022ExplainableAI,
  title = {Explainable AI: Core Ideas, Techniques, and Solutions},
  author = {Dwivedi, Rudresh and Dave, Devam and Naik, Het and others},
  year = {2022},
  doi = {10.1145/3561048},
  journal = {ACM Computing Surveys},
  keywords = {type:survey, xai, interpretability, taxonomy},
  abstract = {This survey reviews explainable AI (XAI) approaches, categorizing them into model-specific and model-agnostic techniques. It highlights trade-offs between accuracy and transparency and proposes a structured taxonomy to support future research.},
  url = {https://dl.acm.org/doi/pdf/10.1145/3561048},
}

@article{Zolanvari2022TrustXAI,
  title = {TRUST XAI: Model-Agnostic Explanations for AI With a Case Study on IIoT Security},
  author = {Zolanvari, Maede and Yang, Zebo and Khan, Khaled and others},
  year = {2022},
  doi = {10.48550/arXiv.2205.01232},
  journal = {arXiv preprint},
  keywords = {type:framework, model_agnostic, cybersecurity, industrial_ai},
  abstract = {The paper proposes TRUST XAI, a model-agnostic explanation system validated on industrial security datasets. The framework integrates statistical methods and visualization tools to improve interpretability in real-world deployments.},
  url = {https://arxiv.org/pdf/2205.01232.pdf},
}

@article{Sulthana2024TrustworthyXAI,
  title = {A Review of Trustworthy and Explainable Artificial Intelligence (XAI)},
  author = {Sulthana, Sulthana and others},
  year = {2024},
  doi = {10.48550/arXiv.2403.14680},
  journal = {arXiv preprint},
  keywords = {type:survey, trustworthy_ai, fairness, governance},
  abstract = {This literature review bridges the gap between explainability and trust in AI by integrating insights from bias mitigation, fairness auditing, and responsible development. A roadmap for future research is proposed.},
  url = {https://arxiv.org/pdf/2403.14680.pdf},
}

@article{Mersha2024NeedsApplications,
  title = {Explainable AI: A Survey of Needs, Techniques, Applications, and Future Direction},
  author = {Mersha, Melkamu and Lam, Khang and Wood, Joseph and Alshami, Ali K.},
  year = {2024},
  doi = {10.1016/j.neucom.2024.128111},
  journal = {Neurocomputing},
  keywords = {type:survey, xai_applications, user_needs, design},
  abstract = {The survey outlines application-specific XAI needs across sectors like education and governance, analyzing gaps in current systems and proposing tailored explanation methods for different user types.},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231224008828/pdfft},
}

@article{Leichtmann2022TrustBehavior,
  title = {Effects of Explainable Artificial Intelligence on Trust and Human Behavior in a High-Risk Decision Task},
  author = {Leichtmann, B. and Humer, C. and Hinterreiter, A. and others},
  year = {2022},
  doi = {10.1016/j.chb.2022.107539},
  journal = {Computers in Human Behavior},
  keywords = {type:user_study, trust, decision_making, behavior},
  abstract = {This empirical study examines how different explanation styles impact user trust and behavior. Contextualized example explanations were found to maintain trust even when system performance declined.},
  url = {https://www.sciencedirect.com/science/article/pii/S0747563222003594/pdfft},
}

@article{Haque2022UserPerspective,
  title = {Explainable Artificial Intelligence (XAI) from a User Perspective},
  author = {Haque, A.K.M.B. and Islam, A.K.M.N. and Mikalef, P.},
  year = {2022},
  doi = {10.1016/j.techfore.2022.122120},
  journal = {Technological Forecasting and Social Change},
  keywords = {type:meta_review, user_experience, xai_framework},
  abstract = {The paper synthesizes prior work to present a user-centered taxonomy of XAI. Key trust drivers identified include explanation clarity, perceived fairness, and cognitive effort required.},
  url = {https://www.sciencedirect.com/science/article/pii/S0040162522006412/pdfft},
}

@article{Afroogh2024VirtualAgents,
  title = {Trust in AI: Progress, Challenges, and Future Directions},
  author = {Afroogh, Saleh and Akbari, Ali and Malone, Evan and others},
  year = {2024},
  doi = {10.1057/s41599-024-04044-8},
  journal = {Humanities and Social Sciences Communications},
  keywords = {type:user_study, trust, dialogue_ai, interaction_design},
  abstract = {Explores how interactive explanation using conversational AI can improve perceived trust in AI systems. Findings suggest users prefer dynamic dialogue over static model descriptions.},
  url = {https://www.nature.com/articles/s41599-024-04044-8.pdf},
}

@article{Ferrario2022JustifiedTrust,
  title = {How Explainability Contributes to Trust in AI},
  author = {Ferrario, Andrea and Loi, Michele},
  year = {2022},
  doi = {10.1145/3531146.3533202},
  journal = {FAccT},
  keywords = {type:conceptual, trust_framework, justification},
  abstract = {Presents a normative framework that distinguishes between transparency and justified trust. The authors argue that explanations must be meaningful to support ethical decision-making.},
  url = {https://facctconference.org/static/pdfs_2022/facct22-3533202.pdf},
}

@article{Atf2024MetaTrust,
  title = {Is Trust Correlated With Explainability in AI? A Meta-Analysis},
  author = {Atf, Zahra and Lewis, Peter R.},
  year = {2024},
  doi = {10.48550/arXiv.2504.12529},
  journal = {arXiv preprint},
  keywords = {type:meta_analysis, trust_explainability_correlation},
  abstract = {A meta-analysis of 38 studies on trust and explainability. Results show variable correlation strength depending on domain, user type, and explanation method.},
  url = {https://arxiv.org/pdf/2504.12529.pdf},
}

@article{Baker2023ResponsibleAI,
  title = {Explainable AI is Responsible AI},
  author = {Baker, Stephanie and Xiang, Wei},
  year = {2023},
  doi = {10.48550/arXiv.2312.01555},
  journal = {arXiv preprint},
  keywords = {type:theoretical, responsible_ai, ethics_policy},
  abstract = {This paper discusses how explainability supports responsible AI design, emphasizing principles such as autonomy, justice, and public accountability. Policy-oriented design recommendations are provided.},
  url = {https://arxiv.org/pdf/2312.01555.pdf},
}

