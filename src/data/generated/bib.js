const generatedBibEntries = {
    "Afroogh2024VirtualAgents": {
        "abstract": "Explores how interactive explanation using conversational AI can improve perceived trust in AI systems. Findings suggest users prefer dynamic dialogue over static model descriptions.",
        "author": "Afroogh, Saleh and Akbari, Ali and Malone, Evan and others",
        "doi": "10.1057/s41599-024-04044-8",
        "journal": "Humanities and Social Sciences Communications",
        "keywords": "type:user_study, trust, dialogue_ai, interaction_design",
        "title": "Trust in AI: Progress, Challenges, and Future Directions",
        "type": "article",
        "url": "https://www.nature.com/articles/s41599-024-04044-8.pdf",
        "year": "2024"
    },
    "Atf2024MetaTrust": {
        "abstract": "A meta-analysis of 38 studies on trust and explainability. Results show variable correlation strength depending on domain, user type, and explanation method.",
        "author": "Atf, Zahra and Lewis, Peter R.",
        "doi": "10.48550/arXiv.2504.12529",
        "journal": "arXiv preprint",
        "keywords": "type:meta_analysis, trust_explainability_correlation",
        "title": "Is Trust Correlated With Explainability in AI? A Meta-Analysis",
        "type": "article",
        "url": "https://arxiv.org/pdf/2504.12529.pdf",
        "year": "2024"
    },
    "Baker2023ResponsibleAI": {
        "abstract": "This paper discusses how explainability supports responsible AI design, emphasizing principles such as autonomy, justice, and public accountability. Policy-oriented design recommendations are provided.",
        "author": "Baker, Stephanie and Xiang, Wei",
        "doi": "10.48550/arXiv.2312.01555",
        "journal": "arXiv preprint",
        "keywords": "type:theoretical, responsible_ai, ethics_policy",
        "title": "Explainable AI is Responsible AI",
        "type": "article",
        "url": "https://arxiv.org/pdf/2312.01555.pdf",
        "year": "2023"
    },
    "Dwivedi2022ExplainableAI": {
        "abstract": "This survey reviews explainable AI (XAI) approaches, categorizing them into model-specific and model-agnostic techniques. It highlights trade-offs between accuracy and transparency and proposes a structured taxonomy to support future research.",
        "author": "Dwivedi, Rudresh and Dave, Devam and Naik, Het and others",
        "doi": "10.1145/3561048",
        "journal": "ACM Computing Surveys",
        "keywords": "type:survey, xai, interpretability, taxonomy",
        "title": "Explainable AI: Core Ideas, Techniques, and Solutions",
        "type": "article",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3561048",
        "year": "2022"
    },
    "Ferrario2022JustifiedTrust": {
        "abstract": "Presents a normative framework that distinguishes between transparency and justified trust. The authors argue that explanations must be meaningful to support ethical decision-making.",
        "author": "Ferrario, Andrea and Loi, Michele",
        "doi": "10.1145/3531146.3533202",
        "journal": "FAccT",
        "keywords": "type:conceptual, trust_framework, justification",
        "title": "How Explainability Contributes to Trust in AI",
        "type": "article",
        "url": "https://facctconference.org/static/pdfs_2022/facct22-3533202.pdf",
        "year": "2022"
    },
    "Haque2022UserPerspective": {
        "abstract": "The paper synthesizes prior work to present a user-centered taxonomy of XAI. Key trust drivers identified include explanation clarity, perceived fairness, and cognitive effort required.",
        "author": "Haque, A.K.M.B. and Islam, A.K.M.N. and Mikalef, P.",
        "doi": "10.1016/j.techfore.2022.122120",
        "journal": "Technological Forecasting and Social Change",
        "keywords": "type:meta_review, user_experience, xai_framework",
        "title": "Explainable Artificial Intelligence (XAI) from a User Perspective",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0040162522006412/pdfft",
        "year": "2022"
    },
    "Leichtmann2022TrustBehavior": {
        "abstract": "This empirical study examines how different explanation styles impact user trust and behavior. Contextualized example explanations were found to maintain trust even when system performance declined.",
        "author": "Leichtmann, B. and Humer, C. and Hinterreiter, A. and others",
        "doi": "10.1016/j.chb.2022.107539",
        "journal": "Computers in Human Behavior",
        "keywords": "type:user_study, trust, decision_making, behavior",
        "title": "Effects of Explainable Artificial Intelligence on Trust and Human Behavior in a High-Risk Decision Task",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0747563222003594/pdfft",
        "year": "2022"
    },
    "Mersha2024NeedsApplications": {
        "abstract": "The survey outlines application-specific XAI needs across sectors like education and governance, analyzing gaps in current systems and proposing tailored explanation methods for different user types.",
        "author": "Mersha, Melkamu and Lam, Khang and Wood, Joseph and Alshami, Ali K.",
        "doi": "10.1016/j.neucom.2024.128111",
        "journal": "Neurocomputing",
        "keywords": "type:survey, xai_applications, user_needs, design",
        "title": "Explainable AI: A Survey of Needs, Techniques, Applications, and Future Direction",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0925231224008828/pdfft",
        "year": "2024"
    },
    "Sulthana2024TrustworthyXAI": {
        "abstract": "This literature review bridges the gap between explainability and trust in AI by integrating insights from bias mitigation, fairness auditing, and responsible development. A roadmap for future research is proposed.",
        "author": "Sulthana, Sulthana and others",
        "doi": "10.48550/arXiv.2403.14680",
        "journal": "arXiv preprint",
        "keywords": "type:survey, trustworthy_ai, fairness, governance",
        "title": "A Review of Trustworthy and Explainable Artificial Intelligence (XAI)",
        "type": "article",
        "url": "https://arxiv.org/pdf/2403.14680.pdf",
        "year": "2024"
    },
    "Zolanvari2022TrustXAI": {
        "abstract": "The paper proposes TRUST XAI, a model-agnostic explanation system validated on industrial security datasets. The framework integrates statistical methods and visualization tools to improve interpretability in real-world deployments.",
        "author": "Zolanvari, Maede and Yang, Zebo and Khan, Khaled and others",
        "doi": "10.48550/arXiv.2205.01232",
        "journal": "arXiv preprint",
        "keywords": "type:framework, model_agnostic, cybersecurity, industrial_ai",
        "title": "TRUST XAI: Model-Agnostic Explanations for AI With a Case Study on IIoT Security",
        "type": "article",
        "url": "https://arxiv.org/pdf/2205.01232.pdf",
        "year": "2022"
    }
};